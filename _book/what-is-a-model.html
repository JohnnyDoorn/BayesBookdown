<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 What is a Model? | A Brief Introduction to Bayesian Inference</title>
  <meta name="description" content="A brief introduction to Bayesian concepts, based on a beer-tasting experiment." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 What is a Model? | A Brief Introduction to Bayesian Inference" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A brief introduction to Bayesian concepts, based on a beer-tasting experiment." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 What is a Model? | A Brief Introduction to Bayesian Inference" />
  
  <meta name="twitter:description" content="A brief introduction to Bayesian concepts, based on a beer-tasting experiment." />
  

<meta name="author" content="Johnny van Doorn" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-lady-tasting-tea.html"/>
<link rel="next" href="model-estimation-section.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Bayesian Introduction</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-lady-tasting-tea.html"><a href="the-lady-tasting-tea.html"><i class="fa fa-check"></i><b>1</b> The Lady Tasting Tea</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-lady-tasting-tea.html"><a href="the-lady-tasting-tea.html#a-bayesian-version"><i class="fa fa-check"></i><b>1.1</b> A Bayesian Version</a></li>
<li class="chapter" data-level="1.2" data-path="the-lady-tasting-tea.html"><a href="the-lady-tasting-tea.html#an-alcoholic-version"><i class="fa fa-check"></i><b>1.2</b> An Alcoholic Version</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="what-is-a-model.html"><a href="what-is-a-model.html"><i class="fa fa-check"></i><b>2</b> What is a Model?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-a-model.html"><a href="what-is-a-model.html#models-make-predictions"><i class="fa fa-check"></i><b>2.1</b> Models Make Predictions</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-a-model.html"><a href="what-is-a-model.html#model-comparison-section"><i class="fa fa-check"></i><b>2.2</b> Model Comparison</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-a-model.html"><a href="what-is-a-model.html#more-models-section"><i class="fa fa-check"></i><b>2.3</b> More Models</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="what-is-a-model.html"><a href="what-is-a-model.html#open-minded-model-section"><i class="fa fa-check"></i><b>2.3.1</b> The Open-Minded Model</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="what-is-a-model.html"><a href="what-is-a-model.html#more-model-comparison-section"><i class="fa fa-check"></i><b>2.4</b> More Model Comparisons</a></li>
<li class="chapter" data-level="2.5" data-path="what-is-a-model.html"><a href="what-is-a-model.html#concluding-thoughts"><i class="fa fa-check"></i><b>2.5</b> Concluding Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="model-estimation-section.html"><a href="model-estimation-section.html"><i class="fa fa-check"></i><b>3</b> How do Models Estimate?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="model-estimation-section.html"><a href="model-estimation-section.html#models-have-beliefs"><i class="fa fa-check"></i><b>3.1</b> Models Have Beliefs</a></li>
<li class="chapter" data-level="3.2" data-path="model-estimation-section.html"><a href="model-estimation-section.html#updating-beliefs"><i class="fa fa-check"></i><b>3.2</b> Updating Beliefs</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="model-estimation-section.html"><a href="model-estimation-section.html#the-likelihood"><i class="fa fa-check"></i><b>3.2.1</b> The Likelihood</a></li>
<li class="chapter" data-level="3.2.2" data-path="model-estimation-section.html"><a href="model-estimation-section.html#the-marginal-likelihood"><i class="fa fa-check"></i><b>3.2.2</b> The Marginal Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="model-estimation-section.html"><a href="model-estimation-section.html#updated-beliefs"><i class="fa fa-check"></i><b>3.3</b> Updated Beliefs</a></li>
<li class="chapter" data-level="3.4" data-path="model-estimation-section.html"><a href="model-estimation-section.html#more-models"><i class="fa fa-check"></i><b>3.4</b> More Models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="model-estimation-section.html"><a href="model-estimation-section.html#sarahs-learning-process"><i class="fa fa-check"></i><b>3.4.1</b> Sarah’s Learning Process</a></li>
<li class="chapter" data-level="3.4.2" data-path="model-estimation-section.html"><a href="model-estimation-section.html#davids-learning-process"><i class="fa fa-check"></i><b>3.4.2</b> David’s Learning Process</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="model-estimation-section.html"><a href="model-estimation-section.html#prior-distribution-in-bayesian-parameter-estimation"><i class="fa fa-check"></i><b>3.5</b> Prior Distribution in Bayesian Parameter Estimation</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="model-estimation-section.html"><a href="model-estimation-section.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.5.1</b> The Beta Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="model-estimation-section.html"><a href="model-estimation-section.html#beta-interpretation-updating-section"><i class="fa fa-check"></i><b>3.5.2</b> Beta Interpretation</a></li>
<li class="chapter" data-level="3.5.3" data-path="model-estimation-section.html"><a href="model-estimation-section.html#two-sided-vs-one-sided-estimation"><i class="fa fa-check"></i><b>3.5.3</b> Two-sided vs One-sided Estimation</a></li>
<li class="chapter" data-level="3.5.4" data-path="model-estimation-section.html"><a href="model-estimation-section.html#an-endless-loop"><i class="fa fa-check"></i><b>3.5.4</b> An Endless Loop</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="model-estimation-section.html"><a href="model-estimation-section.html#relation-to-hypothesis-testing"><i class="fa fa-check"></i><b>3.6</b> Relation to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="model-estimation-section.html"><a href="model-estimation-section.html#the-savage-dickey-density-ratio"><i class="fa fa-check"></i><b>3.6.1</b> The Savage-Dickey Density Ratio</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="model-estimation-section.html"><a href="model-estimation-section.html#concluding-thoughts-1"><i class="fa fa-check"></i><b>3.7</b> Concluding Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-beer-tasting.html"><a href="the-beer-tasting.html"><i class="fa fa-check"></i><b>4</b> The Beer Tasting</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-beer-tasting.html"><a href="the-beer-tasting.html#methods"><i class="fa fa-check"></i><b>4.1</b> Methods</a></li>
<li class="chapter" data-level="4.2" data-path="the-beer-tasting.html"><a href="the-beer-tasting.html#analysis-in-jasp"><i class="fa fa-check"></i><b>4.2</b> Analysis in JASP</a></li>
<li class="chapter" data-level="4.3" data-path="the-beer-tasting.html"><a href="the-beer-tasting.html#continuous-updating"><i class="fa fa-check"></i><b>4.3</b> Continuous Updating</a></li>
<li class="chapter" data-level="4.4" data-path="the-beer-tasting.html"><a href="the-beer-tasting.html#concluding-thoughts-2"><i class="fa fa-check"></i><b>4.4</b> Concluding Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html"><i class="fa fa-check"></i><b>5</b> More Bayesian Analyses</a>
<ul>
<li class="chapter" data-level="5.1" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#the-bayesian-t-test"><i class="fa fa-check"></i><b>5.1</b> The Bayesian T-Test</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#prior-distribution"><i class="fa fa-check"></i><b>5.1.1</b> Prior Distribution</a></li>
<li class="chapter" data-level="5.1.2" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#predictive-updating-factor"><i class="fa fa-check"></i><b>5.1.2</b> Predictive Updating Factor</a></li>
<li class="chapter" data-level="5.1.3" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#posterior-distribution-bayes-factor"><i class="fa fa-check"></i><b>5.1.3</b> Posterior Distribution &amp; Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#the-bayesian-correlation"><i class="fa fa-check"></i><b>5.2</b> The Bayesian Correlation</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#prior-distribution-1"><i class="fa fa-check"></i><b>5.2.1</b> Prior Distribution</a></li>
<li class="chapter" data-level="5.2.2" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#predictive-updating-factor-1"><i class="fa fa-check"></i><b>5.2.2</b> Predictive Updating Factor</a></li>
<li class="chapter" data-level="5.2.3" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#posterior-distribution-bayes-factor-1"><i class="fa fa-check"></i><b>5.2.3</b> Posterior Distribution &amp; Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="more-bayesian-analyses.html"><a href="more-bayesian-analyses.html#concluding-thoughts-3"><i class="fa fa-check"></i><b>5.3</b> Concluding Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>6</b> Exercises</a>
<ul>
<li class="chapter" data-level="6.1" data-path="exercises.html"><a href="exercises.html#binomial-test"><i class="fa fa-check"></i><b>6.1</b> Binomial Test</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="exercises.html"><a href="exercises.html#therapeutic-touch"><i class="fa fa-check"></i><b>6.1.1</b> Therapeutic Touch</a></li>
<li class="chapter" data-level="6.1.2" data-path="exercises.html"><a href="exercises.html#psychologists-tasting-beer"><i class="fa fa-check"></i><b>6.1.2</b> Psychologists Tasting Beer</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="exercises.html"><a href="exercises.html#correlation"><i class="fa fa-check"></i><b>6.2</b> Correlation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="exercises.html"><a href="exercises.html#correlation-a.w.e.s.o.m.-o-4000"><i class="fa fa-check"></i><b>6.2.1</b> Correlation: A.W.E.S.O.M.-O 4000</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="exercises.html"><a href="exercises.html#t-test"><i class="fa fa-check"></i><b>6.3</b> T-Test</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="exercises.html"><a href="exercises.html#the-effect-of-directed-reading-exercises"><i class="fa fa-check"></i><b>6.3.1</b> The Effect of Directed Reading Exercises</a></li>
<li class="chapter" data-level="6.3.2" data-path="exercises.html"><a href="exercises.html#psychologists-tasting-beer-2-t-test-boogaloo"><i class="fa fa-check"></i><b>6.3.2</b> Psychologists Tasting Beer 2: T-Test Boogaloo</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="exercises.html"><a href="exercises.html#summary-stats"><i class="fa fa-check"></i><b>6.4</b> Summary Stats</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="exercises.html"><a href="exercises.html#t-test-flag-priming"><i class="fa fa-check"></i><b>6.4.1</b> T-Test: Flag Priming</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Brief Introduction to Bayesian Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="what-is-a-model" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> What is a Model?<a href="what-is-a-model.html#what-is-a-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Before we dive into the analysis of the beer tasting experiment, we need to define some key components. First of all, the concept of a <em>statistical model</em>. A statistical model is a combination of a general statistical model (e.g., the binomial model) and a statement about a parameter value that describe a certain phenomenon. For instance, we can model the flipping of a fair coin with the binomial model, where the probability parameter <span class="math inline">\(\theta\)</span> (“theta”) is set to <span class="math inline">\(0.5\)</span>. Or, we can model the height of Dutch men (in cm) with a normal model, where the location parameter <span class="math inline">\(\mu = 183\)</span> and the dispersion parameter <span class="math inline">\(\sigma = 5\)</span>. A statistical model can therefore also be seen as a hypothesis: a specific statement about the value of the model parameters.</p>
<div id="models-make-predictions" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Models Make Predictions<a href="what-is-a-model.html#models-make-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
An essential property of a statistical model is that it can make predictions about the real world. We can use the accuracy of these predictions to gauge the quality/plausibility of a model, relative to another model. For instance,
Sarah thinks that the probability of heads in a coin flip is 50% (i.e., <span class="math inline">\(H_S: \theta = 0.5\)</span>), while Paul claims that the coin has been tampered with, and that the probability of heads is 80% (i.e., <span class="math inline">\(H_P: \theta = 0.8\)</span>). Here, Sarah and Paul postulate different models/hypotheses. They are both <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial models</a>, which is the general statistical model for describing a series of chance-based events with a binary outcome (e.g., coin flip, red/black in roulette, whether a random person from the population has a certain disease or not, or someone identifying the alcholic beer). Where Sarah and Paul differ, however, is their claim about the specific value of the <span class="math inline">\(\theta\)</span> parameter. In the remainder of this text, we will be refering to model to mean such a combination of general statistical model, and claim about the value of the model parameter (i.e., hypothesis).
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:two-models-binomial"></span>
<img src="_main_files/figure-html/two-models-binomial-1.svg" alt="Two models for a coin toss. The arrows indicate what each of the models postulate: both postulate a single value for theta." width="90%" />
<p class="caption">
Figure 2.1: Two models for a coin toss. The arrows indicate what each of the models postulate: both postulate a single value for theta.
</p>
</div>
<p>The two models make a different claim about <span class="math inline">\(\theta\)</span>, and therefore also make different <em>predictions</em> about the outcome of a series of 10 coin flips. Specifically, we can use the binomial model to calculate how likely each possible outcome is under each of the models. For instance, we can calculate how likely it is to observe 8 heads out of 10 flips.
The binomial formula is as follows:
<span class="math display">\[\begin{align}
\label{binomFormula}
P(\text{data} \mid \theta) = \frac{n!}{k! (n-k)!} \theta^k\left(1-\theta\right)^{n-k},
\end{align}\]</span>
which, if we fill in the outcome for which we want to know the likelihood (i.e., <span class="math inline">\(k=8\)</span> heads out of <span class="math inline">\(n=10\)</span> flips), becomes:
<span class="math display">\[ P(\text{8 heads out of 10} \mid \theta) = \frac{10!}{8! (10-8)!} \theta^8\left(1-\theta\right)^{10-8}.\]</span>
The last element to fill in is <span class="math inline">\(\theta\)</span>. If we do so for Sarah, who postulates <span class="math inline">\(\theta = 0.5\)</span>, we get 0.0439. For Paul, who postulates <span class="math inline">\(\theta = 0.8\)</span>, we get 0.302. If we do this for every possible outcome, and create a bar graph of each likelihood, we get the following two figures that illustrate what each model deems likely (the yellow bar indicates each models’ likelihood of the example of 8 heads out of 10 flips):</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:two-models-likelihoods-binomial"></span>
<img src="_main_files/figure-html/two-models-likelihoods-binomial-1.png" alt="The likelihoods of all possible outcomes of 10 coin flips, under Sarahs model and under Pauls model. The yellow bar indicates the likelihood of the observed data (8 heads)." width="90%" />
<p class="caption">
Figure 2.2: The likelihoods of all possible outcomes of 10 coin flips, under Sarahs model and under Pauls model. The yellow bar indicates the likelihood of the observed data (8 heads).
</p>
</div>
<!-- These two figures also reflect how Sarah and Paul would spread their money, if they would be handed 100€ and asked to bet on the outcome of 10 coin tosses. While they would bet most of their money on the value exactly equal to $\theta \times n$ (i.e., 5 for Sarah and 8 for Paul), there will always be some random noise in the sample, which causes the observed data to manifest (slightly) differently from the true population value.  -->
<p>These two figures reflect likely outcomes of the experiment of flipping a coin 10 times. If Sarah is correct, and the probability of heads is in fact <span class="math inline">\(0.5\)</span>, likely outcomes are 4, 5, and 6. However, if Paul is correct, and <span class="math inline">\(\theta = 0.8\)</span>, it is more likely to see 7, 8 or 9 heads.</p>
</div>
<div id="model-comparison-section" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Model Comparison<a href="what-is-a-model.html#model-comparison-section" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous section we have made concrete what each of the two models predict. The models differ in their statement about <span class="math inline">\(\theta\)</span> (Figure <a href="what-is-a-model.html#fig:two-models-binomial">2.1</a>), and therefore differ in what they deem likely outcomes (Figure <a href="what-is-a-model.html#fig:two-models-likelihoods-binomial">2.2</a>). Now imagine that we actually gather some data by flipping a coin 10 times, and we observe 8 heads and 2 tails. Figure <a href="what-is-a-model.html#fig:two-models-likelihoods-binomial">2.2</a> tells us that the probability of that happening under Sarahs model is 0.0439, while under Pauls model this is 0.302. These two numbers tell us something about how well each model predicted the data, relative to each other. Specifically, the ratio of these two numbers is known as the <strong>Bayes factor</strong>. Here, the Bayes factor is equal to 0.15, which means that the observed data are about 0.15 times more likely under Sarahs model than under Pauls model.
The Bayes factor has a subscript that indicates what model is being compared to what: <span class="math inline">\(\text{BF}_{SP}\)</span> gives how much more likely Sarahs model is than Pauls, while <span class="math inline">\(\text{BF}_{PS}\)</span> gives how much more likely Pauls is than Sarahs. To go from <span class="math inline">\(\text{BF}_{SP}\)</span> to <span class="math inline">\(\text{BF}_{PS}\)</span>, you simply take 1 divided by the other: <span class="math inline">\(\text{BF}_{PS}\)</span> = <span class="math inline">\(\frac{1}{\text{BF}_{SP}}\)</span> = 6.872. So saying that the data are 6.872 times more likely under Pauls model than Sarahs model is exactly the same. Generally, it is a bit easier to communicate the Bayes factor that is <span class="math inline">\(&gt;1\)</span>, using the appropriate subscript.</p>
<p>Lastly, it can be the case that two models predicted the data equally well. In this case the Bayes factor will be equal to 1. Generally, we want the Bayes factor to be as far away from 1 as possible, since this indicates more and more evidence in favor of one model over another. Different categorizations have been made to translate a Bayes factor into human words, to facilitate communication about degrees of evidence for/against one model respective to another. One such representation is given below in Figure <a href="what-is-a-model.html#fig:bayes-factor-classification">2.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bayes-factor-classification"></span>
<img src="Figures/BF_TableInterpretation.png" alt="A graphical representation of a Bayes factor classification table. As the Bayes factor deviates from 1, which indicates equal support for $H_0$ and $H_1$, more support is gained for either $H_0$ or $H_1$. The probability wheels illustrate the continuous scale of evidence that Bayes factors represent. These classifications are heuristic and should not be misused as an absolute rule for binary all-or-nothing conclusions." width="90%" />
<p class="caption">
Figure 2.3: A graphical representation of a Bayes factor classification table. As the Bayes factor deviates from 1, which indicates equal support for <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span>, more support is gained for either <span class="math inline">\(H_0\)</span> or <span class="math inline">\(H_1\)</span>. The probability wheels illustrate the continuous scale of evidence that Bayes factors represent. These classifications are heuristic and should not be misused as an absolute rule for binary all-or-nothing conclusions.
</p>
</div>
</div>
<div id="more-models-section" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> More Models<a href="what-is-a-model.html#more-models-section" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have considered two models (<span class="math inline">\(H_S\)</span> and <span class="math inline">\(H_P\)</span>), both of which postulate a single value for the model parameter <span class="math inline">\(\theta\)</span>. However, it is possible for a model to be more uncertain in its assertions. For instance, we can have a model that postulates that the probability of heads is greater than <span class="math inline">\(0.5\)</span> (i.e., <span class="math inline">\(0.5 \leq \theta \leq 1\)</span> <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>). This corresponds to the belief that the coin is tampered with, but without making a strong statement about the degree of tampering. Furthermore, next to the model postulating the range for <span class="math inline">\(\theta\)</span>, it also needs to specify how likely it deems every value in this range. Let’s add two more people and their models to the mix to illustrate. Betty believes the coin has been tampered with, but is unsure about the degree of tampering: she believes that every value of <span class="math inline">\(\theta\)</span> between 0.5 and 1 is equally likely. Next is David, who is a bit more extreme in his beliefs: he believes that the coin is tampered with heavily, so assumes that values of <span class="math inline">\(\theta\)</span> close to 1 are more likely than values of <span class="math inline">\(\theta\)</span> closer to <span class="math inline">\(0.5\)</span>.
If we were to plot the models and corresponding hypotheses of Betty and David, they would look as follows (the difference in density reflecting their different beliefs):</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:two-models-binomial-onesided"></span>
<img src="_main_files/figure-html/two-models-binomial-onesided-1.png" alt="Two more models for a coin toss. The colored regions indicate what each model believes. Even though both Betty and David belive the probabilty of heads to be greater than 0.5, they differ in how plausible they deem specific values in that range." width="90%" />
<p class="caption">
Figure 2.4: Two more models for a coin toss. The colored regions indicate what each model believes. Even though both Betty and David belive the probabilty of heads to be greater than 0.5, they differ in how plausible they deem specific values in that range.
</p>
</div>
<p>Compared to the models in Figure <a href="what-is-a-model.html#fig:two-models-binomial">2.1</a>, which only “bet” on a single value, the models above spread their bets more. David and Betty thus make safer bets since they make wider predictions.
Although both Betty and David only consider positive values, they differ in how plausible they deem specific positive values. As before, these models also make predictions about how likely various outcomes of a series of 10 coin flips would be. Again, the binomial formula can be used. However, this time the models do not predict a single value, but a whole range of values. In order to compute how likely Betty’s model deems an outcome of 8 heads out of 10 flips, we have to consider every postulated value of <span class="math inline">\(\theta\)</span> between 0.5 and 1, compute the likelihood of the data for each value, and average across all of these likelihoods, weighted by the density at each point. The technical term for such weighted averaging is called marginalizing, and we refer to this averaged likelihood as the <strong>marginal likelihood</strong>. In the next section we will revisit this topic.</p>
In Figure <a href="what-is-a-model.html#fig:two-models-binomial-onesided-predictions">2.5</a> below, you can see the marginal likelihoods for all outcomes, for each of the two additional models. Note that even though neither Betty nor David postulate values of <span class="math inline">\(\theta\)</span> below 0.5 (i.e., the <em>parameter</em>), they assign some plausibility to observed proportions below 0.5 (i.e., the <em>statistic</em>, or observed data). This reflects the random nature of a coin flip: even though the true probability of heads is <span class="math inline">\(0.6\)</span>, you might still observe 3 heads out of 10 flips.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:two-models-binomial-onesided-predictions"></span>
<img src="_main_files/figure-html/two-models-binomial-onesided-predictions-1.png" alt="The marginal likelihoods of all possible outcomes of 10 coin flips, under the two additional models. The yellow bar indicates the marginal likelihood of the observed data (8 heads)." width="90%" />
<p class="caption">
Figure 2.5: The marginal likelihoods of all possible outcomes of 10 coin flips, under the two additional models. The yellow bar indicates the marginal likelihood of the observed data (8 heads).
</p>
</div>
<div id="open-minded-model-section" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> The Open-Minded Model<a href="what-is-a-model.html#open-minded-model-section" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Lastly, but perhaps most importantly, we can also consider a model that tries to spread its bets as much as possible. Let’s say that Alex wants to keep as much of an open mind about values of <span class="math inline">\(\theta\)</span> as possible. They consider each possible value of <span class="math inline">\(\theta\)</span> to be equally plausible. In Bayesian inference, we also refer to this type of model as the <em>uninformed</em> model. The figure below illustrates what the uninformed model posits, and which outcomes it deems likely. We again have a model that postulates multiple values, so the figure on the right depicts marginal likelihoods. For instance, for the yellow bar, we look at how likely 8 heads out of 10 flips are, averaged over all values postulated by the model, weighted by the density in the left graph.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uninformed-model-binomial-prediction"></span>
<img src="_main_files/figure-html/uninformed-model-binomial-prediction-1.png" alt="The so-called &quot;uninformed model&quot;. Alex wants to keep an open mind about the values of theta and considers each value equally plausible. Left: the colored region indicate what Alex believes. Right: what this specific model considers likely outcomes. The yellow bar indicates the marginal likelihood of the observed data (8 heads)." width="90%" />
<p class="caption">
Figure 2.6: The so-called “uninformed model”. Alex wants to keep an open mind about the values of theta and considers each value equally plausible. Left: the colored region indicate what Alex believes. Right: what this specific model considers likely outcomes. The yellow bar indicates the marginal likelihood of the observed data (8 heads).
</p>
</div>
</div>
</div>
<div id="more-model-comparison-section" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> More Model Comparisons<a href="what-is-a-model.html#more-model-comparison-section" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can apply the same principles from section <a href="what-is-a-model.html#model-comparison-section">2.2</a> to compare how well each of the additional models has predicted the observed data of 8 heads out of 10 coin flips. To do so, we can simply take the ratio of each of the yellow bars in the figures that depict how likely each model considers the various possible outcomes of 10 coin flips. For instance, Alex’ model has a marginal likelihood of 0.0909 for 8 heads, whereas Betty’s model has a marginal likelihood of 0.1756 for 8 heads. If we want to compare the predictions of Betty and Alex, we can look at the ratio of these values to obtain <span class="math inline">\(\text{BF}_{AB} =\)</span> 0.5, which is equivalent to <span class="math inline">\(\text{BF}_{BA} =\)</span> 1.9. This means that the data are about twice as likely under Betty’s model than under Alex’ model, which can be considered weak evidence in favor of Betty’s model over Alex’ model.</p>
<p>If we were to use the betting analogy again, we could say that while both Alex and Betty had bet some money on the outcome of 8 heads, Betty had bet more money on this particular outcome than Alex, and is therefore rewarded more. Because Betty has a more specific belief (namely that the coin is biased towards heads), she had more money at her disposal for betting on the considered values (i.e., values between 0.5 and 1). In contrast, Alex played it very safely: they win some money for any outcome because they spread their betting money across all values. However, because of this, their reward is lower for having correctly predicted the observed data compared to someone who made a more specific bet on the observed data. The phenomenon of more specific models being rewarded more (when predicting well) than their non-specific competitor is known as <strong>parsimony</strong>, and will be discussed in more depth in Chapter 4.</p>
<p>A last model comparison we can make is to compare Alex’ model to Sarah’s model. In a typical (two-sided) statistical test about a proportion, this is the most often-used comparison: Sarah’s model is considered to be the null model, and Alex’ model is considered the two-sided alternative model. As we saw, Alex’ marginal likelihood is 0.0909, while Sarah’s marginal likelihood is 0.0439, so the Bayes factor comparing these two models, <span class="math inline">\(\text{BF}_{AS}\)</span>, equals 2.07. This means the data are about twice as likely under Alex’ model compared to Sarah’s model.</p>
<p>As a bonus, when we know <span class="math inline">\(\text{BF}_{BA}\)</span> and <span class="math inline">\(\text{BF}_{AS}\)</span>, we automatically know <span class="math inline">\(\text{BF}_{BS}\)</span>. Since we know how many more times Betty’s model is than Alex’ (about 2 times), and how many more times Alex’ model is than Sarah’s (about 2 times), we also now know that Betty’s model is about <span class="math inline">\(2 \times 2 = 4\)</span> times more likely than Sarah’s model! This property it known as <strong>transitivity</strong>.</p>
</div>
<div id="concluding-thoughts" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Concluding Thoughts<a href="what-is-a-model.html#concluding-thoughts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we have seen several key ideas:</p>
<ul>
<li>Models make concrete statements about parameters of a model. In this case, about the <span class="math inline">\(\theta\)</span> parameter in the binomial model</li>
<li>These statements can be characterized by a probability distribution, where the probability mass reflects the specific statement
<ul>
<li>The model could hypothesize a single value (e.g., <a href="what-is-a-model.html#fig:two-models-binomial">the models of Sarah and Paul</a>)</li>
<li>The model could hypothesize a range of values (e.g., <a href="what-is-a-model.html#fig:two-models-binomial-onesided">the models of Betty, David</a> and <a href="what-is-a-model.html#fig:uninformed-model-binomial-prediction">Alex</a>)</li>
</ul></li>
<li>After we have observed some data, we can use the Bayes factor to compare the quality of the predictions made by each model
<ul>
<li>The Bayes factor is a relative metric, comparing 2 models at a time</li>
<li>The subscript of the Bayes factor indicates which model is compared to which</li>
<li>More specific predictions, when accurate, are rewarded (parsimony)</li>
</ul></li>
</ul>
<p>Instead of comparing models, however, we can also look at one model, and use it to <strong>estimate</strong> the value of <span class="math inline">\(\theta\)</span>. We will see that each of the models presented above will yield different estimates, because they had different <em>a priori</em> (i.e., before seeing the data) beliefs about plausible values of <span class="math inline">\(\theta\)</span>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Because we are working with a continuous range of values for <span class="math inline">\(\theta\)</span>, the difference between saying <span class="math inline">\(0.5 &lt; \theta\)</span> and <span class="math inline">\(0.5 \leq \theta\)</span> is infinitesimally small and the two versions may be used interchangeably.<a href="what-is-a-model.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-lady-tasting-tea.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-estimation-section.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
